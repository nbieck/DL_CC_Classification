% This section is for the experiment part of the paper, where we talk about the
% pipeline, datasets, and the models we used.

% Suggested structure:
% 1- Experiment setup/structure (pipeline)
% 2- Datasets
% 3- CC models
% 4- CLF models

\section{Experiment}

\subsection{Datasets}

We used the publicly available Oxford 17 \cite{Nilsback06} and Oxford 104 \cite{Nilsback08} datasets. By using only existing datasets, we ensure that our results can more
easily be contrasted with other research in this area.

\subsection{Color Constancy}

\begin{figure}[ht]
    \centering
    \begin{tabular}{c|cccc}
        \includegraphics[width=0.165\textwidth]{cc_demo/flower001_base.png}       &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower001_whitePatch.png} &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower001_greyWorld.png}  &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower001_grayEdge.png}   &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower001_fc4.png}                                \\
        (a)                                                                       & (b) & (c) & (d) & (e) \\
        \includegraphics[width=0.165\textwidth]{cc_demo/flower268_base.png}       &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower268_whitePatch.png} &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower268_greyWorld.png}  &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower268_grayEdge.png}   &
        \includegraphics[width=0.165\textwidth]{cc_demo/flower268_fc4.png}                                \\
        (f)                                                                       & (g) & (h) & (i) & (j)
    \end{tabular}
    \caption{A comparison of color constancy algorithms: (a) and (f): Original image.
        (b) and (g): White Patch. (c) and (h): Grey-World.
        (d) and (i): Grey-Edge. (e) and (j): FC\textsuperscript{4}}
    \label{fig:cc_comparison}
\end{figure}

In order to have a wider spread of examined methods, we both make use of state-of-the art
learning based methods, as well as classical simpler methods
like White-Patch, Grey-World \cite{EbnerConstancy} and Grey-Edge \cite{van2005color}.
A comparison of these can be found in Figure \ref{fig:cc_comparison}.

\subsubsection{Statistical Methods}

In the grey world algorithm, the assumption is made that the average reflectance of the scene should
be a shade of grey. We can therefore infer that any deviation of the average color from this grey tone stems from
the illumination in the scene. By simply dividing this out, we receive a color-corrected image.

White-Patch is a close relative of this method, where instead we assume that the brightest spot in the image (for each channel)
is representative of the overall light color.

In the grey-edge hypothesis we instead assume that the average image gradient can be used as an indication of the light color.

All of these algorithms were reimplemented by us. Notably, based on the recommendation made by Ebner \cite{EbnerConstancy},
after color adjustment we rescale all results such that the top 5\% of values (across all channels) will be clipped to
the maximum intensity of 1.

\subsubsection{Learning Based Model}

We use the FC\textsuperscript{4} model\cite{hu2017fc} as our representative for the deep learning based models. This is a considerably more complicated approach
than the statistical methods listed above, as it makes use of deep convolutional networks.

We chose FC\textsuperscript{4}, as it can provide us with high quality outputs, while still being reasonably simple to use within our framework. While newer
models might perform slightly better when evaluated purely on their results, we judged that FC\textsuperscript{4} should already provide a sufficiently large\
delta to the statistical methods to give an indication of whether this approach is worth pursuing.

\subsection{Classification models}

In this study, two distinct classification models were employed: a Pre-trained VGG16 and a custom-built network developed by us. 
To ensure a standardized approach for fair comparison and evaluation, certain key methodologies were incorporated. 
Specifically, we applied consistent RMSprop optimizer and Sparse cross entropy loss function across both models.
Given that we have 17 and 102 flower categories, this loss function is suitable for our multi-class classification problem. Moreover, to explore different optimization paths, we set the learning rate 
for our custome model as 1e-3 and 1e-4 for the Pre-trained VGG16 model. 

\subsubsection{VGG16}

The VGG16 model, well-known for its simplicity and effectiveness in image classification task, is used as a feature extractor in our study. Pre-trained on the ImageNet dataset, it effectively captures high-level features. 
By employing transfer learning, we take advantage of the pre-trained weights of the VGG16 model that were trained on the large-scale ImageNet dataset. 
The last four layers are fine-tuned for our flower classification, while the first layers remain frozen to preserve latent features. The final layer is replaced with a new fully connected layer of 17 neurons and SoftMax activation function. 
And a flatten layer that converts the output into a one-dimensional feature vector, followed by a dense layer of 1024 neurons and a dropout layer to prevent overfitting. 

\subsubsection{Our Implemented Classifier}

As for our custome classifier, We incorporated. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/experiment_pipeline.png}
    \caption{Pipeline for the experiment.}
    \label{fig:experiment_pipeline}
\end{figure}

The initial version of our network is composed of a convolution layer, followed by max pooling, flatten, and a dense layer, finishing with a softmax activation.
The goal of the initial version is to verify that the dataset was preprocessed in a way that allows fitting and evaluating the model, and to later tune it.
For this, the 17 flowers dataset was split into training and validation sets (with the test set split to be implemented later) in a 80/20 ratio and used to fit the model.

Once the fitting was successful and it was verified that the dataset splits work, we implemented the basis for model tuning using the Keras Tuner \cite{omalley2019kerastuner}.
Number of filters and the kernel size in the convolution layer were the focus of the tuning trial, with values varying from 32 to 512 with steps of 32 for the number of filters, and kernel sizes of 3 and 5.
Number of trials was set to 3, max pooling kernel size of 2x2, Adam optimizer, and Sparse Categorical Crossentropy for the loss function. Trial results are as follows:

\begin{tabular}{c|c|c}
    Filters & Kernel size & Score  \\
    \hline
    \hline
    224     & 3x3         & 0.5515 \\
    \hline
    320     & 5x5         & 0.5000 \\
    \hline
    32      & 5x5         & 0.4890
\end{tabular}

This verifies that we can easily tune our network as we develop it further. It's clear that the score is low and the model can be further improved with additional layers.
Furthermore, it was noted that the model quickly overfits past epoch number $5$ with an accuracy of around $0.99$ but validation score of approximately $0.5$.
The use of, e.g., dropout layers and other techniques could help avoid overfitting.

Next steps include adding more layers to the model, implementing features to avoid overfitting (i.e., dropout), experimenting with a depthwise and depthwise-separable convolution approach, and applying FUNY-NET to the datasets preprocessed by the CC models.